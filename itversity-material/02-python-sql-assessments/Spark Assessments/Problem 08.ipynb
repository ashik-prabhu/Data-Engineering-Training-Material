{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 08\n",
    "\n",
    "Weightage: 25\n",
    "\n",
    "Get bikes based on top 10 monthly trip duration for each month in 2019 first quarter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "All of the citibike trip data is available under **/public/citibike/trips**. It contain multiple folders - one for each month. Here is the schema.\n",
    "\n",
    "```\n",
    "root\n",
    " |-- tripduration: integer (nullable = true)\n",
    " |-- starttime: timestamp (nullable = true)\n",
    " |-- stoptime: timestamp (nullable = true)\n",
    " |-- startstationid: string (nullable = true)\n",
    " |-- endstationid: string (nullable = true)\n",
    " |-- bikeid: integer (nullable = true)\n",
    " |-- usertype: string (nullable = true)\n",
    " |-- birthyear: string (nullable = true)\n",
    " |-- gender: integer (nullable = true)\n",
    " |-- month: integer (nullable = true)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Requirements\n",
    "\n",
    "* Place the result in the HDFS Directory \n",
    "```\n",
    "/user/`whoami`/mock_test_02/problem08/solution\n",
    "```\n",
    "* Use CSV and save the output to exactly one file. Make sure to preserve the header.\n",
    "* Here are the column names. The delimiter should be tab character.\n",
    "```\n",
    " |-- tripmonth: integer (nullable = true)\n",
    " |-- bikeid: integer (nullable = true)\n",
    " |-- tripduration: integer (nullable = true)\n",
    "```\n",
    "* Data should be sorted in ascending order by tripmonth and then in descending order by tripduration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "Here are the self validation steps:\n",
    "* Run the following to check number of files.\n",
    "```\n",
    "hdfs dfs -ls /user/`whoami`/mock_test_02/problem08/solution\n",
    "```\n",
    "* Run this code to create dataframe by name data.\n",
    "```\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "spark.read.csv(f'/user/{username}/mock_test_02/problem08/solution',\n",
    "               sep='\\t',\n",
    "               header=True,\n",
    "               inferSchema=True\n",
    "              ). \\\n",
    "    printSchema()\n",
    "```\n",
    "* Here is the output of the previous code to get the schema details.\n",
    "```\n",
    "root\n",
    " |-- tripmonth: integer (nullable = true)\n",
    " |-- bikeid: integer (nullable = true)\n",
    " |-- tripduration: integer (nullable = true)\n",
    "```\n",
    "* Run the following to validate the data. It should show 21 or more records including header. Validate against the output.\n",
    "```\n",
    "hdfs dfs -cat /user/`whoami`/mock_test_02/problem08/solution/part*\n",
    "```\n",
    "* Output\n",
    "```\n",
    "tripmonth\tbikeid\ttripduration\n",
    "201901\t34105\t2704377\n",
    "201901\t28660\t2410501\n",
    "201901\t19486\t2387390\n",
    "201901\t33699\t2091471\n",
    "201901\t30321\t1818328\n",
    "201901\t20934\t1738805\n",
    "201901\t34355\t1474421\n",
    "201901\t27128\t1397734\n",
    "201901\t32139\t1172618\n",
    "201901\t34183\t1161233\n",
    "201902\t33883\t2422646\n",
    "201902\t21481\t1988809\n",
    "201902\t27447\t1965938\n",
    "201902\t14858\t1463295\n",
    "201902\t35731\t1458714\n",
    "201902\t28660\t1450403\n",
    "201902\t30143\t1401552\n",
    "201902\t29019\t1384035\n",
    "201902\t35035\t1362512\n",
    "201902\t14903\t1322733\n",
    "201903\t30462\t2982161\n",
    "201903\t28384\t2767163\n",
    "201903\t35680\t2606391\n",
    "201903\t25718\t1943545\n",
    "201903\t29770\t1839760\n",
    "201903\t37947\t1630514\n",
    "201903\t34184\t1592415\n",
    "201903\t25441\t1483435\n",
    "201903\t15506\t1462460\n",
    "201903\t31275\t1397484\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    appName(f'Problem 08 | {username}'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_df=spark.read.csv('/public/citibike/trips/',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format,count,lit,col,sum,dense_rank\n",
    "df=trips_df. \\\n",
    "    filter(col('month').between(201901,201903)). \\\n",
    "    select(col('month').cast(\"int\").alias('tripmonth'),col('bikeid').cast(\"int\"),col('tripduration').cast(\"int\")). \\\n",
    "    groupBy('tripmonth','bikeid'). \\\n",
    "    agg(sum(col('tripduration').cast(\"int\")).alias('tripduration')). \\\n",
    "    orderBy('tripmonth',col('tripduration').desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "spec=Window. \\\n",
    "        partitionBy('tripmonth'). \\\n",
    "        orderBy(col('tripduration').desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=df. \\\n",
    "        withColumn('dense_rank',dense_rank().over(spec)). \\\n",
    "        orderBy('tripmonth',col('tripduration').desc()). \\\n",
    "        filter(col('dense_rank')<=10). \\\n",
    "        drop('dense_rank'). \\\n",
    "        orderBy('tripmonth',col('tripduration').desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output. \\\n",
    "    coalesce(1). \\\n",
    "    write. \\\n",
    "    csv('/user/itv002461/mock_test_02/problem08/solution',header=True,mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   3 itv002461 supergroup          0 2022-06-30 04:26 /user/itv002461/mock_test_02/problem08/solution/_SUCCESS\n",
      "-rw-r--r--   3 itv002461 supergroup        660 2022-06-30 04:26 /user/itv002461/mock_test_02/problem08/solution/part-00000-df2ab3c8-1e5a-46fd-8741-c4d2bcfe6105-c000.csv\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "hdfs dfs -ls /user/itv002461/mock_test_02/problem08/solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tripmonth,bikeid,tripduration: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "username = getpass.getuser()\n",
    "spark.read.csv(f'/user/{username}/mock_test_02/problem08/solution',\n",
    "             sep='\\t',\n",
    "             header=True,\n",
    "             inferSchema=True\n",
    "            ). \\\n",
    "  printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tripmonth,bikeid,tripduration\n",
      "201901,34105,2704377\n",
      "201901,28660,2410501\n",
      "201901,19486,2387390\n",
      "201901,33699,2091471\n",
      "201901,30321,1818328\n",
      "201901,20934,1738805\n",
      "201901,34355,1474421\n",
      "201901,27128,1397734\n",
      "201901,32139,1172618\n",
      "201901,34183,1161233\n",
      "201902,33883,2422646\n",
      "201902,21481,1988809\n",
      "201902,27447,1965938\n",
      "201902,14858,1463295\n",
      "201902,35731,1458714\n",
      "201902,28660,1450403\n",
      "201902,30143,1401552\n",
      "201902,29019,1384035\n",
      "201902,35035,1362512\n",
      "201902,14903,1322733\n",
      "201903,30462,2982161\n",
      "201903,28384,2767163\n",
      "201903,35680,2606391\n",
      "201903,25718,1943545\n",
      "201903,29770,1839760\n",
      "201903,37947,1630514\n",
      "201903,34184,1592415\n",
      "201903,25441,1483435\n",
      "201903,15506,1462460\n",
      "201903,31275,1397484\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "hdfs dfs -cat /user/`whoami`/mock_test_02/problem08/solution/part*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
