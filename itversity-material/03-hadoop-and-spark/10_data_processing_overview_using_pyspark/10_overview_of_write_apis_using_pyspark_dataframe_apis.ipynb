{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Spark Write APIs\n",
    "\n",
    "Let us understand how we can write Data Frames to different file formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* All the batch write APIs are grouped under write which is exposed to Data Frame objects.\n",
    "* All APIs are exposed under spark.read\n",
    "  * `text` - to write single column data to text files.\n",
    "  * `csv` - to write to text files with delimiters. Default is a comma, but we can use other delimiters as well.\n",
    "  * `json` - to write data to JSON files\n",
    "  * `orc` - to write data to ORC files\n",
    "  * `parquet` - to write data to Parquet files.\n",
    "* We can also write data to other file formats by plugging in and by using `write.format`, for example **avro**\n",
    "* We can use options based on the type using which we are writing the Data Frame to.\n",
    "  * `compression` - Compression codec (`gzip`, `snappy` etc)\n",
    "  * `sep` - to specify delimiters while writing into text files using **csv**\n",
    "* We can `overwrite` the directories or `append` to existing directories using `mode`\n",
    "* Create copy of orders data in **parquet** file format with no compression. If the folder already exists overwrite it. Target Location: **/user/[YOUR_USER_NAME]/retail_db/orders**\n",
    "* When you pass options, if there are typos then options will be ignored rather than failing. Be careful and make sure that output is validated.\n",
    "* By default the number of files in the output directory is equal to number of tasks that are used to process the data in the last stage. However, we might want to control number of files so that we don't run into too many small files issue.\n",
    "* We can control number of files by using `coalesce`. It has to be invoked on top of Data Frame before invoking `write`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start spark context for this Notebook so that we can execute the code provided. You can sign up for our [10 node state of the art cluster/labs](https://labs.itversity.com/plans) to learn Spark SQL using our unique integrated LMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f'{username} | Python - Data Processing - Overview'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use CLIs, you can use Spark SQL using one of the 3 approaches.\n",
    "\n",
    "**Using Spark SQL**\n",
    "\n",
    "```\n",
    "spark2-sql \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Scala**\n",
    "\n",
    "```\n",
    "spark2-shell \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Pyspark**\n",
    "\n",
    "```\n",
    "pyspark2 \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/itv002461/retail_db\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -rm -R -skipTrash /user/${USER}/retail_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = spark. \\\n",
    "    read. \\\n",
    "    csv('/public/retail_db/orders',\n",
    "        schema='''\n",
    "            order_id INT, \n",
    "            order_date STRING, \n",
    "            order_customer_id INT, \n",
    "            order_status STRING\n",
    "        '''\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|             7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|             4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|             2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|             5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:...|              918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:...|             1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:...|             9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:...|             9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:...|             2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:...|             7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:...|             2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:...|             1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:...|             9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:...|             9198|     PROCESSING|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68883"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders. \\\n",
    "    write. \\\n",
    "    parquet(f'/user/{username}/retail_db/orders', \n",
    "            mode='overwrite', \n",
    "            compression='none'\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   3 itv002461 supergroup          0 2022-06-06 09:38 /user/itv002461/retail_db/orders/_SUCCESS\n",
      "-rw-r--r--   3 itv002461 supergroup     495238 2022-06-06 09:38 /user/itv002461/retail_db/orders/part-00000-ea98f8cc-0f2c-49a3-8b5a-346bbac35d27-c000.parquet\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls /user/${USER}/retail_db/orders\n",
    "\n",
    "# File extension should not contain compression algorithms such as snappy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative approach - using option\n",
    "orders. \\\n",
    "    write. \\\n",
    "    mode('overwrite'). \\\n",
    "    option('compression', 'none'). \\\n",
    "    parquet(f'/user/{username}/retail_db/orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   3 itv002461 supergroup          0 2022-06-06 09:38 /user/itv002461/retail_db/orders/_SUCCESS\n",
      "-rw-r--r--   3 itv002461 supergroup     495238 2022-06-06 09:38 /user/itv002461/retail_db/orders/part-00000-453d51a8-3b7f-4a86-91aa-17a8b723a92c-c000.parquet\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls /user/${USER}/retail_db/orders\n",
    "\n",
    "# File extension should not contain compression algorithms such as snappy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative approach - using format\n",
    "orders. \\\n",
    "    write. \\\n",
    "    mode('overwrite'). \\\n",
    "    option('compression', 'none'). \\\n",
    "    format('parquet'). \\\n",
    "    save(f'/user/{username}/retail_db/orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   3 itv002461 supergroup          0 2022-06-06 09:38 /user/itv002461/retail_db/orders/_SUCCESS\n",
      "-rw-r--r--   3 itv002461 supergroup     495238 2022-06-06 09:38 /user/itv002461/retail_db/orders/part-00000-372fd3ce-45bc-4b54-8e24-c5f51cb1fd81-c000.parquet\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls /user/${USER}/retail_db/orders\n",
    "\n",
    "# File extension should not contain compression algorithms such as snappy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Read order_items data from **/public/retail_db_json/order_items** and write it to pipe delimited files with gzip compression. Target Location: **/user/[YOUR_USER_NAME]/retail_db/order_items**. Make sure to validate.\n",
    "* Ignore the error if the target location already exists. Also make sure to write into only one file. We can use `coalesce` for it. \n",
    "\n",
    "**`coalesce` will be covered in detail at a later point in time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items = spark. \\\n",
    "    read. \\\n",
    "    json('/public/retail_db_json/order_items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "|order_item_id|order_item_order_id|order_item_product_id|order_item_product_price|order_item_quantity|order_item_subtotal|\n",
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "|            1|                  1|                  957|                  299.98|                  1|             299.98|\n",
      "|            2|                  2|                 1073|                  199.99|                  1|             199.99|\n",
      "|            3|                  2|                  502|                    50.0|                  5|              250.0|\n",
      "|            4|                  2|                  403|                  129.99|                  1|             129.99|\n",
      "|            5|                  4|                  897|                   24.99|                  2|              49.98|\n",
      "|            6|                  4|                  365|                   59.99|                  5|             299.95|\n",
      "|            7|                  4|                  502|                    50.0|                  3|              150.0|\n",
      "|            8|                  4|                 1014|                   49.98|                  4|             199.92|\n",
      "|            9|                  5|                  957|                  299.98|                  1|             299.98|\n",
      "|           10|                  5|                  365|                   59.99|                  5|             299.95|\n",
      "|           11|                  5|                 1014|                   49.98|                  2|              99.96|\n",
      "|           12|                  5|                  957|                  299.98|                  1|             299.98|\n",
      "|           13|                  5|                  403|                  129.99|                  1|             129.99|\n",
      "|           14|                  7|                 1073|                  199.99|                  1|             199.99|\n",
      "|           15|                  7|                  957|                  299.98|                  1|             299.98|\n",
      "|           16|                  7|                  926|                   15.99|                  5|              79.95|\n",
      "|           17|                  8|                  365|                   59.99|                  3|             179.97|\n",
      "|           18|                  8|                  365|                   59.99|                  5|             299.95|\n",
      "|           19|                  8|                 1014|                   49.98|                  4|             199.92|\n",
      "|           20|                  8|                  502|                    50.0|                  1|               50.0|\n",
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_items.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_item_id: long (nullable = true)\n",
      " |-- order_item_order_id: long (nullable = true)\n",
      " |-- order_item_product_id: long (nullable = true)\n",
      " |-- order_item_product_price: double (nullable = true)\n",
      " |-- order_item_quantity: long (nullable = true)\n",
      " |-- order_item_subtotal: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_items.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172198"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_items.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using format\n",
    "order_items. \\\n",
    "    coalesce(1). \\\n",
    "    write. \\\n",
    "    mode('ignore'). \\\n",
    "    option('compression', 'gzip'). \\\n",
    "    option('sep', '|'). \\\n",
    "    format('csv'). \\\n",
    "    save(f'/user/{username}/retail_db/order_items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   3 itv002461 supergroup          0 2022-06-06 09:39 /user/itv002461/retail_db/order_items/_SUCCESS\n",
      "-rw-r--r--   3 itv002461 supergroup    1032820 2022-06-06 09:39 /user/itv002461/retail_db/order_items/part-00000-6c2b4139-5d31-4519-853a-0da32a28a5d5-c000.csv.gz\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls /user/${USER}/retail_db/order_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative approach - using keyword arguments\n",
    "order_items. \\\n",
    "    coalesce(1). \\\n",
    "    write. \\\n",
    "    csv(f'/user/{username}/retail_db/order_items',\n",
    "        sep='|',\n",
    "        mode='overwrite',\n",
    "        compression='gzip'\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   3 itv002461 supergroup          0 2022-06-06 09:42 /user/itv002461/retail_db/order_items/_SUCCESS\n",
      "-rw-r--r--   3 itv002461 supergroup    1032820 2022-06-06 09:42 /user/itv002461/retail_db/order_items/part-00000-b197bee8-4cc9-463c-a0ad-63a599f0aef9-c000.csv.gz\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls /user/${USER}/retail_db/order_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
