{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Spark read APIs\n",
    "\n",
    "Let us get the overview of Spark read APIs to read files of different formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `spark` has a bunch of APIs to read data from files of different formats.\n",
    "* All APIs are exposed under `spark.read`\n",
    "  * `text` - to read single column data from text files as well as reading each of the whole text file as one record.\n",
    "  * `csv`- to read text files with delimiters. Default is a comma, but we can use other delimiters as well.\n",
    "  * `json` - to read data from JSON files\n",
    "  * `orc` - to read data from ORC files\n",
    "  * `parquet` - to read data from Parquet files.\n",
    "  * We can also read data from other file formats by plugging in and by using `spark.read.format`\n",
    "* We can also pass options based on the file formats.\n",
    "  * `inferSchema` - to infer the data types of the columns based on the data.\n",
    "  * `header` - to use header to get the column names in case of text files.\n",
    "  * `schema` - to explicitly specify the schema.\n",
    "* We can get the help on APIs like `spark.read.csv` using `help(spark.read.csv)`.\n",
    "* Reading delimited data from text files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start spark context for this Notebook so that we can execute the code provided. You can sign up for our [10 node state of the art cluster/labs](https://labs.itversity.com/plans) to learn Spark SQL using our unique integrated LMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f'{username} | Python - Data Processing - Overview'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use CLIs, you can use Spark SQL using one of the 3 approaches.\n",
    "\n",
    "**Using Spark SQL**\n",
    "\n",
    "```\n",
    "spark2-sql \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Scala**\n",
    "\n",
    "```\n",
    "spark2-shell \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Pyspark**\n",
    "\n",
    "```\n",
    "pyspark2 \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://g01.itversity.com:36619\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>itv002461 | Python - Data Processing - Overview</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fcc0102ac88>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.readwriter.DataFrameReader at 0x7fcc01390eb8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mquote\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mescape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcomment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mignoreLeadingWhiteSpace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mignoreTrailingWhiteSpace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnullValue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnanValue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpositiveInf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnegativeInf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdateFormat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtimestampFormat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmaxColumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmaxCharsPerColumn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmaxMalformedLogPerPartition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcolumnNameOfCorruptRecord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmultiLine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msamplingRatio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0menforceSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0memptyValue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Loads a CSV file and returns the result as a  :class:`DataFrame`.\n",
       "\n",
       "This function will go through the input once to determine the input schema if\n",
       "``inferSchema`` is enabled. To avoid going through the entire data once, disable\n",
       "``inferSchema`` option or specify the schema explicitly using ``schema``.\n",
       "\n",
       ":param path: string, or list of strings, for input path(s),\n",
       "             or RDD of Strings storing CSV rows.\n",
       ":param schema: an optional :class:`pyspark.sql.types.StructType` for the input schema\n",
       "               or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
       ":param sep: sets a single character as a separator for each field and value.\n",
       "            If None is set, it uses the default value, ``,``.\n",
       ":param encoding: decodes the CSV files by the given encoding type. If None is set,\n",
       "                 it uses the default value, ``UTF-8``.\n",
       ":param quote: sets a single character used for escaping quoted values where the\n",
       "              separator can be part of the value. If None is set, it uses the default\n",
       "              value, ``\"``. If you would like to turn off quotations, you need to set an\n",
       "              empty string.\n",
       ":param escape: sets a single character used for escaping quotes inside an already\n",
       "               quoted value. If None is set, it uses the default value, ``\\``.\n",
       ":param comment: sets a single character used for skipping lines beginning with this\n",
       "                character. By default (None), it is disabled.\n",
       ":param header: uses the first line as names of columns. If None is set, it uses the\n",
       "               default value, ``false``.\n",
       ":param inferSchema: infers the input schema automatically from data. It requires one extra\n",
       "               pass over the data. If None is set, it uses the default value, ``false``.\n",
       ":param enforceSchema: If it is set to ``true``, the specified or inferred schema will be\n",
       "                      forcibly applied to datasource files, and headers in CSV files will be\n",
       "                      ignored. If the option is set to ``false``, the schema will be\n",
       "                      validated against all headers in CSV files or the first header in RDD\n",
       "                      if the ``header`` option is set to ``true``. Field names in the schema\n",
       "                      and column names in CSV headers are checked by their positions\n",
       "                      taking into account ``spark.sql.caseSensitive``. If None is set,\n",
       "                      ``true`` is used by default. Though the default value is ``true``,\n",
       "                      it is recommended to disable the ``enforceSchema`` option\n",
       "                      to avoid incorrect results.\n",
       ":param ignoreLeadingWhiteSpace: A flag indicating whether or not leading whitespaces from\n",
       "                                values being read should be skipped. If None is set, it\n",
       "                                uses the default value, ``false``.\n",
       ":param ignoreTrailingWhiteSpace: A flag indicating whether or not trailing whitespaces from\n",
       "                                 values being read should be skipped. If None is set, it\n",
       "                                 uses the default value, ``false``.\n",
       ":param nullValue: sets the string representation of a null value. If None is set, it uses\n",
       "                  the default value, empty string. Since 2.0.1, this ``nullValue`` param\n",
       "                  applies to all supported types including the string type.\n",
       ":param nanValue: sets the string representation of a non-number value. If None is set, it\n",
       "                 uses the default value, ``NaN``.\n",
       ":param positiveInf: sets the string representation of a positive infinity value. If None\n",
       "                    is set, it uses the default value, ``Inf``.\n",
       ":param negativeInf: sets the string representation of a negative infinity value. If None\n",
       "                    is set, it uses the default value, ``Inf``.\n",
       ":param dateFormat: sets the string that indicates a date format. Custom date formats\n",
       "                   follow the formats at ``java.text.SimpleDateFormat``. This\n",
       "                   applies to date type. If None is set, it uses the\n",
       "                   default value, ``yyyy-MM-dd``.\n",
       ":param timestampFormat: sets the string that indicates a timestamp format. Custom date\n",
       "                        formats follow the formats at ``java.text.SimpleDateFormat``.\n",
       "                        This applies to timestamp type. If None is set, it uses the\n",
       "                        default value, ``yyyy-MM-dd'T'HH:mm:ss.SSSXXX``.\n",
       ":param maxColumns: defines a hard limit of how many columns a record can have. If None is\n",
       "                   set, it uses the default value, ``20480``.\n",
       ":param maxCharsPerColumn: defines the maximum number of characters allowed for any given\n",
       "                          value being read. If None is set, it uses the default value,\n",
       "                          ``-1`` meaning unlimited length.\n",
       ":param maxMalformedLogPerPartition: this parameter is no longer used since Spark 2.2.0.\n",
       "                                    If specified, it is ignored.\n",
       ":param mode: allows a mode for dealing with corrupt records during parsing. If None is\n",
       "             set, it uses the default value, ``PERMISSIVE``. Note that Spark tries to\n",
       "             parse only required columns in CSV under column pruning. Therefore, corrupt\n",
       "             records can be different based on required set of fields. This behavior can\n",
       "             be controlled by ``spark.sql.csv.parser.columnPruning.enabled``\n",
       "             (enabled by default).\n",
       "\n",
       "        * ``PERMISSIVE`` : when it meets a corrupted record, puts the malformed string \\\n",
       "          into a field configured by ``columnNameOfCorruptRecord``, and sets other \\\n",
       "          fields to ``null``. To keep corrupt records, an user can set a string type \\\n",
       "          field named ``columnNameOfCorruptRecord`` in an user-defined schema. If a \\\n",
       "          schema does not have the field, it drops corrupt records during parsing. \\\n",
       "          A record with less/more tokens than schema is not a corrupted record to CSV. \\\n",
       "          When it meets a record having fewer tokens than the length of the schema, \\\n",
       "          sets ``null`` to extra fields. When the record has more tokens than the \\\n",
       "          length of the schema, it drops extra tokens.\n",
       "        * ``DROPMALFORMED`` : ignores the whole corrupted records.\n",
       "        * ``FAILFAST`` : throws an exception when it meets corrupted records.\n",
       "\n",
       ":param columnNameOfCorruptRecord: allows renaming the new field having malformed string\n",
       "                                  created by ``PERMISSIVE`` mode. This overrides\n",
       "                                  ``spark.sql.columnNameOfCorruptRecord``. If None is set,\n",
       "                                  it uses the value specified in\n",
       "                                  ``spark.sql.columnNameOfCorruptRecord``.\n",
       ":param multiLine: parse records, which may span multiple lines. If None is\n",
       "                  set, it uses the default value, ``false``.\n",
       ":param charToEscapeQuoteEscaping: sets a single character used for escaping the escape for\n",
       "                                  the quote character. If None is set, the default value is\n",
       "                                  escape character when escape and quote characters are\n",
       "                                  different, ``\\0`` otherwise.\n",
       ":param samplingRatio: defines fraction of rows used for schema inferring.\n",
       "                      If None is set, it uses the default value, ``1.0``.\n",
       ":param emptyValue: sets the string representation of an empty value. If None is set, it uses\n",
       "                   the default value, empty string.\n",
       "\n",
       ">>> df = spark.read.csv('python/test_support/sql/ages.csv')\n",
       ">>> df.dtypes\n",
       "[('_c0', 'string'), ('_c1', 'string')]\n",
       ">>> rdd = sc.textFile('python/test_support/sql/ages.csv')\n",
       ">>> df2 = spark.read.csv(rdd)\n",
       ">>> df2.dtypes\n",
       "[('_c0', 'string'), ('_c1', 'string')]\n",
       "\n",
       ".. versionadded:: 2.0\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark-2.4.7-bin-hadoop2.7/python/pyspark/sql/readwriter.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.read.csv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method csv in module pyspark.sql.readwriter:\n",
      "\n",
      "csv(path, schema=None, sep=None, encoding=None, quote=None, escape=None, comment=None, header=None, inferSchema=None, ignoreLeadingWhiteSpace=None, ignoreTrailingWhiteSpace=None, nullValue=None, nanValue=None, positiveInf=None, negativeInf=None, dateFormat=None, timestampFormat=None, maxColumns=None, maxCharsPerColumn=None, maxMalformedLogPerPartition=None, mode=None, columnNameOfCorruptRecord=None, multiLine=None, charToEscapeQuoteEscaping=None, samplingRatio=None, enforceSchema=None, emptyValue=None) method of pyspark.sql.readwriter.DataFrameReader instance\n",
      "    Loads a CSV file and returns the result as a  :class:`DataFrame`.\n",
      "    \n",
      "    This function will go through the input once to determine the input schema if\n",
      "    ``inferSchema`` is enabled. To avoid going through the entire data once, disable\n",
      "    ``inferSchema`` option or specify the schema explicitly using ``schema``.\n",
      "    \n",
      "    :param path: string, or list of strings, for input path(s),\n",
      "                 or RDD of Strings storing CSV rows.\n",
      "    :param schema: an optional :class:`pyspark.sql.types.StructType` for the input schema\n",
      "                   or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
      "    :param sep: sets a single character as a separator for each field and value.\n",
      "                If None is set, it uses the default value, ``,``.\n",
      "    :param encoding: decodes the CSV files by the given encoding type. If None is set,\n",
      "                     it uses the default value, ``UTF-8``.\n",
      "    :param quote: sets a single character used for escaping quoted values where the\n",
      "                  separator can be part of the value. If None is set, it uses the default\n",
      "                  value, ``\"``. If you would like to turn off quotations, you need to set an\n",
      "                  empty string.\n",
      "    :param escape: sets a single character used for escaping quotes inside an already\n",
      "                   quoted value. If None is set, it uses the default value, ``\\``.\n",
      "    :param comment: sets a single character used for skipping lines beginning with this\n",
      "                    character. By default (None), it is disabled.\n",
      "    :param header: uses the first line as names of columns. If None is set, it uses the\n",
      "                   default value, ``false``.\n",
      "    :param inferSchema: infers the input schema automatically from data. It requires one extra\n",
      "                   pass over the data. If None is set, it uses the default value, ``false``.\n",
      "    :param enforceSchema: If it is set to ``true``, the specified or inferred schema will be\n",
      "                          forcibly applied to datasource files, and headers in CSV files will be\n",
      "                          ignored. If the option is set to ``false``, the schema will be\n",
      "                          validated against all headers in CSV files or the first header in RDD\n",
      "                          if the ``header`` option is set to ``true``. Field names in the schema\n",
      "                          and column names in CSV headers are checked by their positions\n",
      "                          taking into account ``spark.sql.caseSensitive``. If None is set,\n",
      "                          ``true`` is used by default. Though the default value is ``true``,\n",
      "                          it is recommended to disable the ``enforceSchema`` option\n",
      "                          to avoid incorrect results.\n",
      "    :param ignoreLeadingWhiteSpace: A flag indicating whether or not leading whitespaces from\n",
      "                                    values being read should be skipped. If None is set, it\n",
      "                                    uses the default value, ``false``.\n",
      "    :param ignoreTrailingWhiteSpace: A flag indicating whether or not trailing whitespaces from\n",
      "                                     values being read should be skipped. If None is set, it\n",
      "                                     uses the default value, ``false``.\n",
      "    :param nullValue: sets the string representation of a null value. If None is set, it uses\n",
      "                      the default value, empty string. Since 2.0.1, this ``nullValue`` param\n",
      "                      applies to all supported types including the string type.\n",
      "    :param nanValue: sets the string representation of a non-number value. If None is set, it\n",
      "                     uses the default value, ``NaN``.\n",
      "    :param positiveInf: sets the string representation of a positive infinity value. If None\n",
      "                        is set, it uses the default value, ``Inf``.\n",
      "    :param negativeInf: sets the string representation of a negative infinity value. If None\n",
      "                        is set, it uses the default value, ``Inf``.\n",
      "    :param dateFormat: sets the string that indicates a date format. Custom date formats\n",
      "                       follow the formats at ``java.text.SimpleDateFormat``. This\n",
      "                       applies to date type. If None is set, it uses the\n",
      "                       default value, ``yyyy-MM-dd``.\n",
      "    :param timestampFormat: sets the string that indicates a timestamp format. Custom date\n",
      "                            formats follow the formats at ``java.text.SimpleDateFormat``.\n",
      "                            This applies to timestamp type. If None is set, it uses the\n",
      "                            default value, ``yyyy-MM-dd'T'HH:mm:ss.SSSXXX``.\n",
      "    :param maxColumns: defines a hard limit of how many columns a record can have. If None is\n",
      "                       set, it uses the default value, ``20480``.\n",
      "    :param maxCharsPerColumn: defines the maximum number of characters allowed for any given\n",
      "                              value being read. If None is set, it uses the default value,\n",
      "                              ``-1`` meaning unlimited length.\n",
      "    :param maxMalformedLogPerPartition: this parameter is no longer used since Spark 2.2.0.\n",
      "                                        If specified, it is ignored.\n",
      "    :param mode: allows a mode for dealing with corrupt records during parsing. If None is\n",
      "                 set, it uses the default value, ``PERMISSIVE``. Note that Spark tries to\n",
      "                 parse only required columns in CSV under column pruning. Therefore, corrupt\n",
      "                 records can be different based on required set of fields. This behavior can\n",
      "                 be controlled by ``spark.sql.csv.parser.columnPruning.enabled``\n",
      "                 (enabled by default).\n",
      "    \n",
      "            * ``PERMISSIVE`` : when it meets a corrupted record, puts the malformed string \\\n",
      "              into a field configured by ``columnNameOfCorruptRecord``, and sets other \\\n",
      "              fields to ``null``. To keep corrupt records, an user can set a string type \\\n",
      "              field named ``columnNameOfCorruptRecord`` in an user-defined schema. If a \\\n",
      "              schema does not have the field, it drops corrupt records during parsing. \\\n",
      "              A record with less/more tokens than schema is not a corrupted record to CSV. \\\n",
      "              When it meets a record having fewer tokens than the length of the schema, \\\n",
      "              sets ``null`` to extra fields. When the record has more tokens than the \\\n",
      "              length of the schema, it drops extra tokens.\n",
      "            * ``DROPMALFORMED`` : ignores the whole corrupted records.\n",
      "            * ``FAILFAST`` : throws an exception when it meets corrupted records.\n",
      "    \n",
      "    :param columnNameOfCorruptRecord: allows renaming the new field having malformed string\n",
      "                                      created by ``PERMISSIVE`` mode. This overrides\n",
      "                                      ``spark.sql.columnNameOfCorruptRecord``. If None is set,\n",
      "                                      it uses the value specified in\n",
      "                                      ``spark.sql.columnNameOfCorruptRecord``.\n",
      "    :param multiLine: parse records, which may span multiple lines. If None is\n",
      "                      set, it uses the default value, ``false``.\n",
      "    :param charToEscapeQuoteEscaping: sets a single character used for escaping the escape for\n",
      "                                      the quote character. If None is set, the default value is\n",
      "                                      escape character when escape and quote characters are\n",
      "                                      different, ``\\0`` otherwise.\n",
      "    :param samplingRatio: defines fraction of rows used for schema inferring.\n",
      "                          If None is set, it uses the default value, ``1.0``.\n",
      "    :param emptyValue: sets the string representation of an empty value. If None is set, it uses\n",
      "                       the default value, empty string.\n",
      "    \n",
      "    >>> df = spark.read.csv('python/test_support/sql/ages.csv')\n",
      "    >>> df.dtypes\n",
      "    [('_c0', 'string'), ('_c1', 'string')]\n",
      "    >>> rdd = sc.textFile('python/test_support/sql/ages.csv')\n",
      "    >>> df2 = spark.read.csv(rdd)\n",
      "    >>> df2.dtypes\n",
      "    [('_c0', 'string'), ('_c1', 'string')]\n",
      "    \n",
      "    .. versionadded:: 2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark.read.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|             7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|             4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|             2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|             5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:...|              918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:...|             1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:...|             9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:...|             9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:...|             2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:...|             7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:...|             2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:...|             1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:...|             9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:...|             9198|     PROCESSING|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark. \\\n",
    "    read. \\\n",
    "    csv('/public/retail_db/orders',\n",
    "        schema='''\n",
    "            order_id INT, \n",
    "            order_date STRING, \n",
    "            order_customer_id INT, \n",
    "            order_status STRING\n",
    "        '''\n",
    "       ). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Reading JSON data from text files. We can infer schema from the data as each JSON object contain both column name and value.\n",
    "* Example for JSON\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"order_id\": 1, \n",
    "    \"order_date\": \"2013-07-25 00:00:00.0\", \n",
    "    \"order_customer_id\": 12345, \n",
    "    \"order_status\": \"COMPLETE\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprimitivesAsString\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprefersDecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mallowComments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mallowUnquotedFieldNames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mallowSingleQuotes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mallowNumericLeadingZero\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mallowBackslashEscapingAnyCharacter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcolumnNameOfCorruptRecord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdateFormat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtimestampFormat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmultiLine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mallowUnquotedControlChars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlineSep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msamplingRatio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdropFieldIfAllNull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Loads JSON files and returns the results as a :class:`DataFrame`.\n",
       "\n",
       "`JSON Lines <http://jsonlines.org/>`_ (newline-delimited JSON) is supported by default.\n",
       "For JSON (one record per file), set the ``multiLine`` parameter to ``true``.\n",
       "\n",
       "If the ``schema`` parameter is not specified, this function goes\n",
       "through the input once to determine the input schema.\n",
       "\n",
       ":param path: string represents path to the JSON dataset, or a list of paths,\n",
       "             or RDD of Strings storing JSON objects.\n",
       ":param schema: an optional :class:`pyspark.sql.types.StructType` for the input schema or\n",
       "               a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
       ":param primitivesAsString: infers all primitive values as a string type. If None is set,\n",
       "                           it uses the default value, ``false``.\n",
       ":param prefersDecimal: infers all floating-point values as a decimal type. If the values\n",
       "                       do not fit in decimal, then it infers them as doubles. If None is\n",
       "                       set, it uses the default value, ``false``.\n",
       ":param allowComments: ignores Java/C++ style comment in JSON records. If None is set,\n",
       "                      it uses the default value, ``false``.\n",
       ":param allowUnquotedFieldNames: allows unquoted JSON field names. If None is set,\n",
       "                                it uses the default value, ``false``.\n",
       ":param allowSingleQuotes: allows single quotes in addition to double quotes. If None is\n",
       "                                set, it uses the default value, ``true``.\n",
       ":param allowNumericLeadingZero: allows leading zeros in numbers (e.g. 00012). If None is\n",
       "                                set, it uses the default value, ``false``.\n",
       ":param allowBackslashEscapingAnyCharacter: allows accepting quoting of all character\n",
       "                                           using backslash quoting mechanism. If None is\n",
       "                                           set, it uses the default value, ``false``.\n",
       ":param mode: allows a mode for dealing with corrupt records during parsing. If None is\n",
       "             set, it uses the default value, ``PERMISSIVE``.\n",
       "\n",
       "        * ``PERMISSIVE`` : when it meets a corrupted record, puts the malformed string                   into a field configured by ``columnNameOfCorruptRecord``, and sets other                   fields to ``null``. To keep corrupt records, an user can set a string type                   field named ``columnNameOfCorruptRecord`` in an user-defined schema. If a                   schema does not have the field, it drops corrupt records during parsing.                   When inferring a schema, it implicitly adds a ``columnNameOfCorruptRecord``                   field in an output schema.\n",
       "        *  ``DROPMALFORMED`` : ignores the whole corrupted records.\n",
       "        *  ``FAILFAST`` : throws an exception when it meets corrupted records.\n",
       "\n",
       ":param columnNameOfCorruptRecord: allows renaming the new field having malformed string\n",
       "                                  created by ``PERMISSIVE`` mode. This overrides\n",
       "                                  ``spark.sql.columnNameOfCorruptRecord``. If None is set,\n",
       "                                  it uses the value specified in\n",
       "                                  ``spark.sql.columnNameOfCorruptRecord``.\n",
       ":param dateFormat: sets the string that indicates a date format. Custom date formats\n",
       "                   follow the formats at ``java.text.SimpleDateFormat``. This\n",
       "                   applies to date type. If None is set, it uses the\n",
       "                   default value, ``yyyy-MM-dd``.\n",
       ":param timestampFormat: sets the string that indicates a timestamp format. Custom date\n",
       "                        formats follow the formats at ``java.text.SimpleDateFormat``.\n",
       "                        This applies to timestamp type. If None is set, it uses the\n",
       "                        default value, ``yyyy-MM-dd'T'HH:mm:ss.SSSXXX``.\n",
       ":param multiLine: parse one record, which may span multiple lines, per file. If None is\n",
       "                  set, it uses the default value, ``false``.\n",
       ":param allowUnquotedControlChars: allows JSON Strings to contain unquoted control\n",
       "                                  characters (ASCII characters with value less than 32,\n",
       "                                  including tab and line feed characters) or not.\n",
       ":param encoding: allows to forcibly set one of standard basic or extended encoding for\n",
       "                 the JSON files. For example UTF-16BE, UTF-32LE. If None is set,\n",
       "                 the encoding of input JSON will be detected automatically\n",
       "                 when the multiLine option is set to ``true``.\n",
       ":param lineSep: defines the line separator that should be used for parsing. If None is\n",
       "                set, it covers all ``\\r``, ``\\r\\n`` and ``\\n``.\n",
       ":param samplingRatio: defines fraction of input JSON objects used for schema inferring.\n",
       "                      If None is set, it uses the default value, ``1.0``.\n",
       ":param dropFieldIfAllNull: whether to ignore column of all null values or empty\n",
       "                           array/struct during schema inference. If None is set, it\n",
       "                           uses the default value, ``false``.\n",
       "\n",
       ">>> df1 = spark.read.json('python/test_support/sql/people.json')\n",
       ">>> df1.dtypes\n",
       "[('age', 'bigint'), ('name', 'string')]\n",
       ">>> rdd = sc.textFile('python/test_support/sql/people.json')\n",
       ">>> df2 = spark.read.json(rdd)\n",
       ">>> df2.dtypes\n",
       "[('age', 'bigint'), ('name', 'string')]\n",
       "\n",
       ".. versionadded:: 1.4\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark-2.4.7-bin-hadoop2.7/python/pyspark/sql/readwriter.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.read.json?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------+---------------+\n",
      "|order_customer_id|          order_date|order_id|   order_status|\n",
      "+-----------------+--------------------+--------+---------------+\n",
      "|            11599|2013-07-25 00:00:...|       1|         CLOSED|\n",
      "|              256|2013-07-25 00:00:...|       2|PENDING_PAYMENT|\n",
      "|            12111|2013-07-25 00:00:...|       3|       COMPLETE|\n",
      "|             8827|2013-07-25 00:00:...|       4|         CLOSED|\n",
      "|            11318|2013-07-25 00:00:...|       5|       COMPLETE|\n",
      "|             7130|2013-07-25 00:00:...|       6|       COMPLETE|\n",
      "|             4530|2013-07-25 00:00:...|       7|       COMPLETE|\n",
      "|             2911|2013-07-25 00:00:...|       8|     PROCESSING|\n",
      "|             5657|2013-07-25 00:00:...|       9|PENDING_PAYMENT|\n",
      "|             5648|2013-07-25 00:00:...|      10|PENDING_PAYMENT|\n",
      "|              918|2013-07-25 00:00:...|      11| PAYMENT_REVIEW|\n",
      "|             1837|2013-07-25 00:00:...|      12|         CLOSED|\n",
      "|             9149|2013-07-25 00:00:...|      13|PENDING_PAYMENT|\n",
      "|             9842|2013-07-25 00:00:...|      14|     PROCESSING|\n",
      "|             2568|2013-07-25 00:00:...|      15|       COMPLETE|\n",
      "|             7276|2013-07-25 00:00:...|      16|PENDING_PAYMENT|\n",
      "|             2667|2013-07-25 00:00:...|      17|       COMPLETE|\n",
      "|             1205|2013-07-25 00:00:...|      18|         CLOSED|\n",
      "|             9488|2013-07-25 00:00:...|      19|PENDING_PAYMENT|\n",
      "|             9198|2013-07-25 00:00:...|      20|     PROCESSING|\n",
      "+-----------------+--------------------+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark. \\\n",
    "    read. \\\n",
    "    json('/public/retail_db_json/orders'). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
