{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order of execution of SQL\n",
    "\n",
    "Let us review the order of execution of SQL. First let us review the order of writing the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start spark context for this Notebook so that we can execute the code provided. You can sign up for our [10 node state of the art cluster/labs](https://labs.itversity.com/plans) to learn Spark SQL using our unique integrated LMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "username = itv002461\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "itv002461"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val username = System.getProperty(\"user.name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "username = itv002461\n",
       "spark = org.apache.spark.sql.SparkSession@42cecd7e\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@42cecd7e"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val username = System.getProperty(\"user.name\")\n",
    "val spark = SparkSession.\n",
    "    builder.\n",
    "    config(\"spark.ui.port\", \"0\").\n",
    "    config(\"spark.sql.warehouse.dir\", s\"/user/${username}/warehouse\").\n",
    "    enableHiveSupport.\n",
    "    appName(s\"${username} | Spark SQL - Windowing Functions\").\n",
    "    master(\"yarn\").\n",
    "    getOrCreate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use CLIs, you can use Spark SQL using one of the 3 approaches.\n",
    "\n",
    "**Using Spark SQL**\n",
    "\n",
    "```\n",
    "spark2-sql \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Scala**\n",
    "\n",
    "```\n",
    "spark2-shell \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Pyspark**\n",
    "\n",
    "```\n",
    "pyspark2 \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **SELECT**\n",
    "2. **FROM**\n",
    "3. **JOIN** or **OUTER JOIN** with **ON**\n",
    "4. **WHERE**\n",
    "5. **GROUP BY** and optionally **HAVING**\n",
    "6. **ORDER BY**\n",
    "\n",
    "Let us come up with a query which will compute daily revenue using COMPLETE or CLOSED orders and also sorted by order_date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++\n",
       "\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "USE itv002461_retail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+--------------------+--------+\n",
       "|          order_date| revenue|\n",
       "+--------------------+--------+\n",
       "|2013-07-25 00:00:...|31547.23|\n",
       "|2013-07-26 00:00:...|54713.23|\n",
       "|2013-07-27 00:00:...|48411.48|\n",
       "|2013-07-28 00:00:...|35672.03|\n",
       "|2013-07-29 00:00:...| 54579.7|\n",
       "|2013-07-30 00:00:...|49329.29|\n",
       "|2013-07-31 00:00:...|59212.49|\n",
       "|2013-08-01 00:00:...|49160.08|\n",
       "|2013-08-02 00:00:...|50688.58|\n",
       "|2013-08-03 00:00:...|43416.74|\n",
       "+--------------------+--------+\n",
       "\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT o.order_date,\n",
    "  round(sum(oi.order_item_subtotal), 2) AS revenue\n",
    "FROM orders o JOIN order_items oi\n",
    "ON o.order_id = oi.order_item_order_id\n",
    "WHERE o.order_status IN ('COMPLETE', 'CLOSED')\n",
    "GROUP BY o.order_date\n",
    "ORDER BY o.order_date\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+--------------------+--------+\n",
       "|          order_date| revenue|\n",
       "+--------------------+--------+\n",
       "|2013-07-26 00:00:...|54713.23|\n",
       "|2013-07-29 00:00:...| 54579.7|\n",
       "|2013-07-31 00:00:...|59212.49|\n",
       "|2013-08-02 00:00:...|50688.58|\n",
       "|2013-08-06 00:00:...|57843.89|\n",
       "|2013-08-12 00:00:...|59014.74|\n",
       "|2013-08-17 00:00:...|63226.83|\n",
       "|2013-08-24 00:00:...|52650.15|\n",
       "|2013-09-05 00:00:...|59942.43|\n",
       "|2013-09-06 00:00:...| 61976.1|\n",
       "+--------------------+--------+\n",
       "\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT o.order_date,\n",
    "    round(sum(oi.order_item_subtotal), 2) AS revenue\n",
    "FROM orders o JOIN order_items oi\n",
    "ON o.order_id = oi.order_item_order_id\n",
    "WHERE o.order_status IN ('COMPLETE', 'CLOSED')\n",
    "GROUP BY o.order_date\n",
    "    HAVING round(sum(oi.order_item_subtotal), 2) >= 50000\n",
    "ORDER BY order_date\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However order of execution is typically as follows.\n",
    "\n",
    "1. **FROM**\n",
    "2. **JOIN** or **OUTER JOIN** with **ON**\n",
    "3. **WHERE**\n",
    "4. **GROUP BY** and optionally **HAVING**\n",
    "5. **SELECT**\n",
    "6. **ORDER BY**\n",
    "\n",
    "As **SELECT** is executed before **ORDER BY** clause, we will not be able to refer the aliases defined in **SELECT** caluse in other clauses except for **ORDER BY** in most of the traditional databases. However, in Spark we can specify the aliases defined in **SELECT** in **HAVING** as well as **ORDER BY**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{error}\n",
    "This will fail as revenue which is an alias defined in **SELECT** cannot be used in **WHERE**.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "               :  +- Su...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Magic sql failed to execute with error: \n",
       "cannot resolve '`revenue`' given input columns: [oi.order_item_quantity, o.order_customer_id, oi.order_item_id, o.order_date, oi.order_item_order_id, oi.order_item_product_price, oi.order_item_subtotal, oi.order_item_product_id, o.order_id, o.order_status]; line 1 pos 202;\n",
       "'GlobalLimit 10\n",
       "+- 'LocalLimit 10\n",
       "   +- 'Sort ['order_date ASC NULLS FIRST], true\n",
       "      +- 'Aggregate ['o.order_date], ['o.order_date, 'round('sum('oi.order_item_subtotal), 2) AS revenue#114]\n",
       "         +- 'Filter (order_status#118 IN (COMPLETE,CLOSED) && ('revenue >= 50000))\n",
       "            +- Join Inner, (order_id#115 = order_item_order_id#120)\n",
       "               :- SubqueryAlias `o`\n",
       "               :  +- SubqueryAlias `itv002461_retail`.`orders`\n",
       "               :     +- HiveTableRelation `itv002461_retail`.`orders`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [order_id#115, order_date#116, order_customer_id#117, order_status#118]\n",
       "               +- SubqueryAlias `oi`\n",
       "                  +- SubqueryAlias `itv002461_retail`.`order_items`\n",
       "                     +- Relation[order_item_id#119,order_item_order_id#120,order_item_product_id#121,order_item_quantity#122,order_item_subtotal#123,order_item_product_price#124] parquet\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT o.order_date,\n",
    "    round(sum(oi.order_item_subtotal), 2) AS revenue\n",
    "FROM orders o JOIN order_items oi\n",
    "ON o.order_id = oi.order_item_order_id\n",
    "WHERE o.order_status IN ('COMPLETE', 'CLOSED')\n",
    "    AND revenue >= 50000\n",
    "GROUP BY o.order_date\n",
    "ORDER BY order_date\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "This will work as revenue which is an alias defined in **SELECT** can be used in **HAVING** as well as **ORDER BY**.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+--------------------+--------+\n",
       "|          order_date| revenue|\n",
       "+--------------------+--------+\n",
       "|2013-07-26 00:00:...|54713.23|\n",
       "|2013-07-29 00:00:...| 54579.7|\n",
       "|2013-07-31 00:00:...|59212.49|\n",
       "|2013-08-02 00:00:...|50688.58|\n",
       "|2013-08-06 00:00:...|57843.89|\n",
       "|2013-08-12 00:00:...|59014.74|\n",
       "|2013-08-17 00:00:...|63226.83|\n",
       "|2013-08-24 00:00:...|52650.15|\n",
       "|2013-09-05 00:00:...|59942.43|\n",
       "|2013-09-06 00:00:...| 61976.1|\n",
       "+--------------------+--------+\n",
       "\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT o.order_date,\n",
    "    round(sum(oi.order_item_subtotal), 2) AS revenue\n",
    "FROM orders o JOIN order_items oi\n",
    "ON o.order_id = oi.order_item_order_id\n",
    "WHERE o.order_status IN ('COMPLETE', 'CLOSED')\n",
    "GROUP BY o.order_date\n",
    "    HAVING revenue >= 50000\n",
    "ORDER BY order_date,\n",
    "    revenue DESC\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 2 - Scala",
   "language": "scala",
   "name": "spark_2_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
