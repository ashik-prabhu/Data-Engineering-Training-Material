{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Windowing Functions\n",
    "\n",
    "Let us get an overview of Analytics or Windowing Functions in Spark SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start spark context for this Notebook so that we can execute the code provided. You can sign up for our [10 node state of the art cluster/labs](https://labs.itversity.com/plans) to learn Spark SQL using our unique integrated LMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "username = itv002461\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "itv002461"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val username = System.getProperty(\"user.name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "username = itv002461\n",
       "spark = org.apache.spark.sql.SparkSession@62a7f2b\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@62a7f2b"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val username = System.getProperty(\"user.name\")\n",
    "val spark = SparkSession.\n",
    "    builder.\n",
    "    config(\"spark.ui.port\", \"0\").\n",
    "    config(\"spark.sql.warehouse.dir\", s\"/user/${username}/warehouse\").\n",
    "    enableHiveSupport.\n",
    "    appName(s\"${username} | Spark SQL - Windowing Functions\").\n",
    "    master(\"yarn\").\n",
    "    getOrCreate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use CLIs, you can use Spark SQL using one of the 3 approaches.\n",
    "\n",
    "**Using Spark SQL**\n",
    "\n",
    "```\n",
    "spark2-sql \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Scala**\n",
    "\n",
    "```\n",
    "spark2-shell \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Pyspark**\n",
    "\n",
    "```\n",
    "pyspark2 \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Aggregate Functions (`sum`, `min`, `max`, `avg`)\n",
    "* Window Functions (`lead`, `lag`, `first_value`, `last_value`)\n",
    "* Rank Functions (`rank`, `dense_rank`, `row_number` etc)\n",
    "* For all the functions we use `OVER` clause.\n",
    "* For aggregate functions we typically use `PARTITION BY`\n",
    "* For global ranking and windowing functions we can use `ORDER BY sorting_column` and for ranking and windowing with in a partition or group we can use `PARTITION BY partition_column ORDER BY sorting_column`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++\n",
       "\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "USE itv002461_hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+-----------+-------------+--------+\n",
       "|employee_id|department_id|  salary|\n",
       "+-----------+-------------+--------+\n",
       "|        155|           80| 7000.00|\n",
       "|        156|           80|10000.00|\n",
       "|        157|           80| 9500.00|\n",
       "|        158|           80| 9000.00|\n",
       "|        159|           80| 8000.00|\n",
       "|        160|           80| 7500.00|\n",
       "|        161|           80| 7000.00|\n",
       "|        162|           80|10500.00|\n",
       "|        163|           80| 9500.00|\n",
       "|        164|           80| 7200.00|\n",
       "+-----------+-------------+--------+\n",
       "\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT employee_id, department_id, salary FROM employees LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "|        105|           60| 4800.00|             5| 59|        106|  ...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "+-----------+-------------+--------+--------------+---+-----------+------------+\n",
       "|employee_id|department_id|  salary|employee_count|rnk|lead_emp_id|lead_emp_sal|\n",
       "+-----------+-------------+--------+--------------+---+-----------+------------+\n",
       "|        100|           90|24000.00|             3|  1|        101|    17000.00|\n",
       "|        101|           90|17000.00|             3|  2|        102|    17000.00|\n",
       "|        102|           90|17000.00|             3|  2|       null|        null|\n",
       "|        103|           60| 9000.00|             5| 24|        104|     6000.00|\n",
       "|        104|           60| 6000.00|             5| 56|        105|     4800.00|\n",
       "|        105|           60| 4800.00|             5| 59|        106|     4800.00|\n",
       "|        106|           60| 4800.00|             5| 59|        107|     4200.00|\n",
       "|        107|           60| 4200.00|             5| 62|       null|        null|\n",
       "|        108|          100|12000.00|             6|  7|        109|     9000.00|\n",
       "|        109|          100| 9000.00|             6| 24|        110|     8200.00|\n",
       "+-----------+-------------+--------+--------------+---+-----------+------------+\n",
       "only showing top 10 rows\n",
       "\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT employee_id, department_id, salary,\n",
    "    count(1) OVER (PARTITION BY department_id) AS employee_count,\n",
    "    rank() OVER (ORDER BY salary DESC) AS rnk,\n",
    "    lead(employee_id) OVER (PARTITION BY department_id ORDER BY salary DESC) AS lead_emp_id,\n",
    "    lead(salary) OVER (PARTITION BY department_id ORDER BY salary DESC) AS lead_emp_sal\n",
    "FROM employees\n",
    "ORDER BY employee_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 2 - Scala",
   "language": "scala",
   "name": "spark_2_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
