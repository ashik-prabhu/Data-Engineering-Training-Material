{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating Data\n",
    "\n",
    "Let us understand how to aggregate the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start spark context for this Notebook so that we can execute the code provided. You can sign up for our [10 node state of the art cluster/labs](https://labs.itversity.com/plans) to learn Spark SQL using our unique integrated LMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "username = itv002461\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "itv002461"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val username = System.getProperty(\"user.name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "username = itv002461\n",
       "spark = org.apache.spark.sql.SparkSession@430240f3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@430240f3"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val username = System.getProperty(\"user.name\")\n",
    "val spark = SparkSession.\n",
    "    builder.\n",
    "    config(\"spark.ui.port\", \"0\").\n",
    "    config(\"spark.sql.warehouse.dir\", s\"/user/${username}/warehouse\").\n",
    "    enableHiveSupport.\n",
    "    appName(s\"${username} | Spark SQL - Basic Transformations\").\n",
    "    master(\"yarn\").\n",
    "    getOrCreate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use CLIs, you can use Spark SQL using one of the 3 approaches.\n",
    "\n",
    "**Using Spark SQL**\n",
    "\n",
    "```\n",
    "spark2-sql \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Scala**\n",
    "\n",
    "```\n",
    "spark2-shell \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Pyspark**\n",
    "\n",
    "```\n",
    "pyspark2 \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can perform global aggregations as well as aggregations by key.\n",
    "* Global Aggregations\n",
    "  * Get total number of orders.\n",
    "  * Get revenue for a given order id.\n",
    "  * Get number of records with order_status either COMPLETED or CLOSED.\n",
    "* Aggregations by key - using `GROUP BY`\n",
    "  * Get number of orders by date or status.\n",
    "  * Get revenue for each order_id.\n",
    "  * Get daily product revenue (using order date and product id as keys).\n",
    "* We can also use `HAVING` clause to apply filtering on top of aggregated data.\n",
    "  * Get daily product revenue where revenue is greater than $500 (using order date and product id as keys).\n",
    "* Rules while using `GROUP BY`.\n",
    "  * We can have the columns which are specified as part of `GROUP BY` in `SELECT` clause.\n",
    "  * On top of those, we can have derived columns using aggregate functions.\n",
    "  * We cannot have any other columns that are not used as part of `GROUP BY` on derived column using non aggregate functions.\n",
    "  * We will not be able to use aggregate functions or aliases used in the select clause as part of the where clause.\n",
    "  * If we want to filter based on aggregated results, then we can leverage `HAVING` on top of `GROUP BY` (specifying `WHERE` is not an option)\n",
    "* Typical query execution - FROM -> WHERE -> GROUP BY -> SELECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++\n",
       "\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql \n",
    "use itv002461_retail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+---------------+\n",
       "|count(order_id)|\n",
       "+---------------+\n",
       "|          68883|\n",
       "+---------------+\n",
       "\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT count(order_id) FROM orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+--------------------------+\n",
       "|count(DISTINCT order_date)|\n",
       "+--------------------------+\n",
       "|                       364|\n",
       "+--------------------------+\n",
       "\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT count(DISTINCT order_date) FROM orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+-------------+\n",
       "|order_revenue|\n",
       "+-------------+\n",
       "|       579.98|\n",
       "+-------------+\n",
       "\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT round(sum(order_item_subtotal), 2) AS order_revenue\n",
    "FROM order_items \n",
    "WHERE order_item_order_id = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+--------+\n",
       "|count(1)|\n",
       "+--------+\n",
       "|   30455|\n",
       "+--------+\n",
       "\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT count(1) \n",
    "FROM orders\n",
    "WHERE order_status IN ('COMPLETE', 'CLOSED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+--------------------+--------+\n",
       "|          order_date|count(1)|\n",
       "+--------------------+--------+\n",
       "|2013-08-13 00:00:...|      73|\n",
       "|2014-03-19 00:00:...|     130|\n",
       "|2014-04-26 00:00:...|     251|\n",
       "|2013-10-12 00:00:...|     162|\n",
       "|2013-11-15 00:00:...|     135|\n",
       "|2013-09-16 00:00:...|     121|\n",
       "|2013-09-20 00:00:...|     139|\n",
       "|2013-12-31 00:00:...|     266|\n",
       "|2014-06-15 00:00:...|     128|\n",
       "|2013-09-06 00:00:...|     276|\n",
       "+--------------------+--------+\n",
       "only showing top 10 rows\n",
       "\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT order_date,\n",
    "    count(1)\n",
    "FROM orders\n",
    "GROUP BY order_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+---------------+------------+\n",
       "|   order_status|status_count|\n",
       "+---------------+------------+\n",
       "|PENDING_PAYMENT|       15030|\n",
       "|       COMPLETE|       22899|\n",
       "|        ON_HOLD|        3798|\n",
       "| PAYMENT_REVIEW|         729|\n",
       "|     PROCESSING|        8275|\n",
       "|         CLOSED|        7556|\n",
       "|SUSPECTED_FRAUD|        1558|\n",
       "|        PENDING|        7610|\n",
       "|       CANCELED|        1428|\n",
       "+---------------+------------+\n",
       "\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT order_status,\n",
    "    count(1) AS status_count\n",
    "FROM orders\n",
    "GROUP BY order_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+-------------------+-------------+\n",
       "|order_item_order_id|order_revenue|\n",
       "+-------------------+-------------+\n",
       "|                148|       479.99|\n",
       "|                463|       829.92|\n",
       "|                471|       169.98|\n",
       "|                496|       441.95|\n",
       "|               1088|       249.97|\n",
       "|               1580|       299.95|\n",
       "|               1591|       439.86|\n",
       "|               1645|      1509.79|\n",
       "|               2366|       299.97|\n",
       "|               2659|       724.91|\n",
       "+-------------------+-------------+\n",
       "\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT order_item_order_id,\n",
    "    round(sum(order_item_subtotal), 2) AS order_revenue\n",
    "FROM order_items\n",
    "GROUP BY order_item_order_id LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+--------------------+-----...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "+--------------------+---------------------+-------+\n",
       "|          order_date|order_item_product_id|revenue|\n",
       "+--------------------+---------------------+-------+\n",
       "|2014-03-11 00:00:...|                  135|  132.0|\n",
       "|2014-03-26 00:00:...|                  897|  24.99|\n",
       "|2014-04-02 00:00:...|                 1073|6399.68|\n",
       "|2014-04-04 00:00:...|                  565|  210.0|\n",
       "|2014-05-07 00:00:...|                  642|  120.0|\n",
       "|2014-05-18 00:00:...|                  835| 127.96|\n",
       "|2014-05-28 00:00:...|                  191|5399.46|\n",
       "|2014-06-02 00:00:...|                  627|  799.8|\n",
       "|2014-06-05 00:00:...|                  627|1079.73|\n",
       "|2014-06-18 00:00:...|                  273| 111.96|\n",
       "+--------------------+---------------------+-------+\n",
       "\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT o.order_date,\n",
    "    oi.order_item_product_id,\n",
    "    round(sum(oi.order_item_subtotal), 2) AS revenue\n",
    "FROM orders o JOIN order_items oi\n",
    "    ON o.order_id = oi.order_item_order_id\n",
    "WHERE o.order_status IN ('COMPLETE', 'CLOSED')\n",
    "GROUP BY o.order_date,\n",
    "    oi.order_item_product_id\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            :  +- SubqueryAlia...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Magic sql failed to execute with error: \n",
       "cannot resolve '`revenue`' given input columns: [oi.order_item_quantity, oi.order_item_id, o.order_date, oi.order_item_product_price, o.order_customer_id, oi.order_item_subtotal, oi.order_item_order_id, o.order_id, o.order_status, oi.order_item_product_id]; line 1 pos 236;\n",
       "'GlobalLimit 10\n",
       "+- 'LocalLimit 10\n",
       "   +- 'Aggregate ['o.order_date, 'oi.order_item_product_id], ['o.order_date, 'oi.order_item_product_id, 'round('sum('oi.order_item_subtotal), 2) AS revenue#494]\n",
       "      +- 'Filter (order_status#498 IN (COMPLETE,CLOSED) && ('revenue >= 500))\n",
       "         +- Join Inner, (order_id#495 = order_item_order_id#500)\n",
       "            :- SubqueryAlias `o`\n",
       "            :  +- SubqueryAlias `itv002461_retail`.`orders`\n",
       "            :     +- HiveTableRelation `itv002461_retail`.`orders`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [order_id#495, order_date#496, order_customer_id#497, order_status#498]\n",
       "            +- SubqueryAlias `oi`\n",
       "               +- SubqueryAlias `itv002461_retail`.`order_items`\n",
       "                  +- HiveTableRelation `itv002461_retail`.`order_items`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [order_item_id#499, order_item_order_id#500, order_item_product_id#501, order_item_quantity#502, order_item_subtotal#503, order_item_product_price#504]\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT o.order_date,\n",
    "    oi.order_item_product_id,\n",
    "    round(sum(oi.order_item_subtotal), 2) AS revenue\n",
    "FROM orders o JOIN order_items oi\n",
    "    ON o.order_id = oi.order_item_order_id\n",
    "WHERE o.order_status IN ('COMPLETE', 'CLOSED')\n",
    "    AND revenue >= 500\n",
    "GROUP BY o.order_date,\n",
    "    oi.order_item_product_id\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+--------------------+-----...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "+--------------------+---------------------+-------+\n",
       "|          order_date|order_item_product_id|revenue|\n",
       "+--------------------+---------------------+-------+\n",
       "|2014-04-09 00:00:...|                  191|6599.34|\n",
       "|2014-05-29 00:00:...|                 1014|2698.92|\n",
       "|2014-06-01 00:00:...|                 1073|3199.84|\n",
       "|2014-06-30 00:00:...|                  502| 4000.0|\n",
       "|2013-08-12 00:00:...|                  627| 3199.2|\n",
       "|2013-09-04 00:00:...|                  957|3599.76|\n",
       "|2013-10-19 00:00:...|                 1004|5599.72|\n",
       "|2013-11-06 00:00:...|                  502| 3800.0|\n",
       "|2014-02-13 00:00:...|                  403|4159.68|\n",
       "|2014-03-06 00:00:...|                  191| 4999.5|\n",
       "+--------------------+---------------------+-------+\n",
       "\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT o.order_date,\n",
    "    oi.order_item_product_id,\n",
    "    round(sum(oi.order_item_subtotal), 2) AS revenue\n",
    "FROM orders o JOIN order_items oi\n",
    "    ON o.order_id = oi.order_item_order_id\n",
    "WHERE o.order_status IN ('COMPLETE', 'CLOSED')\n",
    "GROUP BY o.order_date,\n",
    "    oi.order_item_product_id\n",
    "HAVING revenue >= 500\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using Spark SQL with Python or Scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|count(order_id)|\n",
      "+---------------+\n",
      "|          68883|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT count(order_id) FROM orders\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|count(DISTINCT order_date)|\n",
      "+--------------------------+\n",
      "|                       364|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT count(DISTINCT order_date) FROM orders\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|order_revenue|\n",
      "+-------------+\n",
      "|       579.98|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT round(sum(order_item_subtotal), 2) AS order_revenue\n",
    "FROM order_items \n",
    "WHERE order_item_order_id = 2\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   30455|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT count(1) \n",
    "FROM orders\n",
    "WHERE order_status IN ('COMPLETE', 'CLOSED')\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|          order_date|count(1)|\n",
      "+--------------------+--------+\n",
      "|2013-08-13 00:00:...|      73|\n",
      "|2013-10-12 00:00:...|     162|\n",
      "|2013-11-15 00:00:...|     135|\n",
      "|2014-03-19 00:00:...|     130|\n",
      "|2014-04-26 00:00:...|     251|\n",
      "|2013-09-16 00:00:...|     121|\n",
      "|2013-09-20 00:00:...|     139|\n",
      "|2013-12-31 00:00:...|     266|\n",
      "|2014-06-15 00:00:...|     128|\n",
      "|2013-09-06 00:00:...|     276|\n",
      "|2013-12-24 00:00:...|     170|\n",
      "|2014-06-07 00:00:...|     191|\n",
      "|2014-01-07 00:00:...|     163|\n",
      "|2013-10-14 00:00:...|     139|\n",
      "|2013-11-11 00:00:...|     246|\n",
      "|2014-01-27 00:00:...|     163|\n",
      "|2014-01-29 00:00:...|     158|\n",
      "|2014-02-14 00:00:...|     174|\n",
      "|2014-04-15 00:00:...|     180|\n",
      "|2014-04-22 00:00:...|     144|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT order_date,\n",
    "    count(1)\n",
    "FROM orders\n",
    "GROUP BY order_date\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+\n",
      "|   order_status|status_count|\n",
      "+---------------+------------+\n",
      "|PENDING_PAYMENT|       15030|\n",
      "|       COMPLETE|       22899|\n",
      "|        ON_HOLD|        3798|\n",
      "| PAYMENT_REVIEW|         729|\n",
      "|     PROCESSING|        8275|\n",
      "|         CLOSED|        7556|\n",
      "|SUSPECTED_FRAUD|        1558|\n",
      "|        PENDING|        7610|\n",
      "|       CANCELED|        1428|\n",
      "+---------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT order_status,\n",
    "    count(1) AS status_count\n",
    "FROM orders\n",
    "GROUP BY order_status\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+\n",
      "|order_item_order_id|order_revenue|\n",
      "+-------------------+-------------+\n",
      "|              35351|       629.93|\n",
      "|              35361|       614.88|\n",
      "|              35689|       799.88|\n",
      "|              35694|       889.94|\n",
      "|              35820|       949.96|\n",
      "|              35912|       799.96|\n",
      "|              35947|       519.96|\n",
      "|              35982|      1201.82|\n",
      "|              36131|       554.95|\n",
      "|              36224|       729.96|\n",
      "|              36538|       299.98|\n",
      "|              37111|       479.92|\n",
      "|              37146|       329.98|\n",
      "|              37251|       689.87|\n",
      "|              37307|       849.94|\n",
      "|              37489|       329.94|\n",
      "|              38153|       761.91|\n",
      "|              38220|       689.89|\n",
      "|              38311|        99.98|\n",
      "|              38422|      1249.91|\n",
      "+-------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT order_item_order_id,\n",
    "    round(sum(order_item_subtotal), 2) AS order_revenue\n",
    "FROM order_items\n",
    "GROUP BY order_item_order_id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+-------+\n",
      "|          order_date|order_item_product_id|revenue|\n",
      "+--------------------+---------------------+-------+\n",
      "|2014-03-28 00:00:...|                  793|  59.96|\n",
      "|2014-04-09 00:00:...|                  191|6599.34|\n",
      "|2014-04-10 00:00:...|                  775|   9.99|\n",
      "|2014-04-15 00:00:...|                  116| 404.91|\n",
      "|2014-05-03 00:00:...|                  172|  120.0|\n",
      "|2014-05-24 00:00:...|                  249| 164.91|\n",
      "|2014-05-29 00:00:...|                 1014|2698.92|\n",
      "|2014-06-01 00:00:...|                 1073|3199.84|\n",
      "|2014-06-06 00:00:...|                  810|  39.98|\n",
      "|2014-06-08 00:00:...|                  792|  89.94|\n",
      "|2014-06-30 00:00:...|                  502| 4000.0|\n",
      "|2014-07-01 00:00:...|                  926|  31.98|\n",
      "|2014-07-02 00:00:...|                  793|  14.99|\n",
      "|2013-08-12 00:00:...|                  627| 3199.2|\n",
      "|2013-09-04 00:00:...|                  957|3599.76|\n",
      "|2013-10-19 00:00:...|                 1004|5599.72|\n",
      "|2013-11-06 00:00:...|                  502| 3800.0|\n",
      "|2013-12-20 00:00:...|                  305|  199.0|\n",
      "|2014-02-13 00:00:...|                  403|4159.68|\n",
      "|2014-06-09 00:00:...|                  691| 399.95|\n",
      "+--------------------+---------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT o.order_date,\n",
    "    oi.order_item_product_id,\n",
    "    round(sum(oi.order_item_subtotal), 2) AS revenue\n",
    "FROM orders o JOIN order_items oi\n",
    "    ON o.order_id = oi.order_item_order_id\n",
    "WHERE o.order_status IN ('COMPLETE', 'CLOSED')\n",
    "GROUP BY o.order_date,\n",
    "    oi.order_item_product_id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": "cannot resolve '`revenue`' given input columns: [oi.order_item_product_id, o.order_customer_id, oi.order_item_product_price, o.order_id, oi.order_item_id, oi.order_item_subtotal, o.order_date, oi.order_item_order_id, oi.order_item_quantity, o.order_status]; line 8 pos 8;\n'Aggregate ['o.order_date, 'oi.order_item_product_id], ['o.order_date, 'oi.order_item_product_id, 'round('sum('oi.order_item_subtotal), 2) AS revenue#672]\n+- 'Filter (order_status#676 IN (COMPLETE,CLOSED) && ('revenue >= 500))\n   +- Join Inner, (order_id#673 = order_item_order_id#678)\n      :- SubqueryAlias `o`\n      :  +- SubqueryAlias `itv002461_retail`.`orders`\n      :     +- HiveTableRelation `itv002461_retail`.`orders`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [order_id#673, order_date#674, order_customer_id#675, order_status#676]\n      +- SubqueryAlias `oi`\n         +- SubqueryAlias `itv002461_retail`.`order_items`\n            +- HiveTableRelation `itv002461_retail`.`order_items`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [order_item_id#677, order_item_order_id#678, order_item_product_id#679, order_item_quantity#680, order_item_subtotal#681, order_item_product_price#682]\n",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: cannot resolve '`revenue`' given input columns: [oi.order_item_product_id, o.order_customer_id, oi.order_item_product_price, o.order_id, oi.order_item_id, oi.order_item_subtotal, o.order_date, oi.order_item_order_id, oi.order_item_quantity, o.order_status]; line 8 pos 8;",
      "'Aggregate ['o.order_date, 'oi.order_item_product_id], ['o.order_date, 'oi.order_item_product_id, 'round('sum('oi.order_item_subtotal), 2) AS revenue#672]",
      "+- 'Filter (order_status#676 IN (COMPLETE,CLOSED) && ('revenue >= 500))",
      "   +- Join Inner, (order_id#673 = order_item_order_id#678)",
      "      :- SubqueryAlias `o`",
      "      :  +- SubqueryAlias `itv002461_retail`.`orders`",
      "      :     +- HiveTableRelation `itv002461_retail`.`orders`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [order_id#673, order_date#674, order_customer_id#675, order_status#676]",
      "      +- SubqueryAlias `oi`",
      "         +- SubqueryAlias `itv002461_retail`.`order_items`",
      "            +- HiveTableRelation `itv002461_retail`.`order_items`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [order_item_id#677, order_item_order_id#678, order_item_product_id#679, order_item_quantity#680, order_item_subtotal#681, order_item_product_price#682]",
      "  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:279)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)",
      "  at scala.collection.immutable.List.foreach(List.scala:392)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:125)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)",
      "  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)",
      "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)",
      "  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:643)",
      "  ... 42 elided"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT o.order_date,\n",
    "    oi.order_item_product_id,\n",
    "    round(sum(oi.order_item_subtotal), 2) AS revenue\n",
    "FROM orders o JOIN order_items oi\n",
    "    ON o.order_id = oi.order_item_order_id\n",
    "WHERE o.order_status IN ('COMPLETE', 'CLOSED')\n",
    "    AND revenue >= 500\n",
    "GROUP BY o.order_date,\n",
    "    oi.order_item_product_id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+-------+\n",
      "|          order_date|order_item_product_id|revenue|\n",
      "+--------------------+---------------------+-------+\n",
      "|2014-04-09 00:00:...|                  191|6599.34|\n",
      "|2014-05-29 00:00:...|                 1014|2698.92|\n",
      "|2014-06-01 00:00:...|                 1073|3199.84|\n",
      "|2014-06-30 00:00:...|                  502| 4000.0|\n",
      "|2013-08-12 00:00:...|                  627| 3199.2|\n",
      "|2013-09-04 00:00:...|                  957|3599.76|\n",
      "|2013-10-19 00:00:...|                 1004|5599.72|\n",
      "|2013-11-06 00:00:...|                  502| 3800.0|\n",
      "|2014-02-13 00:00:...|                  403|4159.68|\n",
      "|2013-08-04 00:00:...|                  403|3119.76|\n",
      "|2013-09-08 00:00:...|                 1073|3199.84|\n",
      "|2013-09-09 00:00:...|                  957|5399.64|\n",
      "|2013-09-15 00:00:...|                  627|1039.74|\n",
      "|2013-09-22 00:00:...|                  365|6058.99|\n",
      "|2013-10-03 00:00:...|                  565|  700.0|\n",
      "|2013-11-03 00:00:...|                  957|8099.46|\n",
      "|2013-11-25 00:00:...|                 1014|2798.88|\n",
      "|2013-12-19 00:00:...|                  191|2399.76|\n",
      "|2013-12-22 00:00:...|                  403|5069.61|\n",
      "|2013-12-28 00:00:...|                  403|3769.71|\n",
      "+--------------------+---------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT o.order_date,\n",
    "    oi.order_item_product_id,\n",
    "    round(sum(oi.order_item_subtotal), 2) AS revenue\n",
    "FROM orders o JOIN order_items oi\n",
    "    ON o.order_id = oi.order_item_order_id\n",
    "WHERE o.order_status IN ('COMPLETE', 'CLOSED')\n",
    "GROUP BY o.order_date,\n",
    "    oi.order_item_product_id\n",
    "HAVING revenue >= 500\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Spark 2 - Scala",
   "language": "scala",
   "name": "spark_2_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
