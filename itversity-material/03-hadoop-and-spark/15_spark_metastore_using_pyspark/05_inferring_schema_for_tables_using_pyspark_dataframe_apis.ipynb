{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferring Schema for Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we want to create a table using `spark.catalog.createTable` or using `spark.catalog.createExternalTable`, we need to specify Schema.\n",
    "\n",
    "* Schema can be inferred from the Dataframe and then can be passed using `StructType` object while creating the table.\n",
    "* `StructType` takes list of objects of type `StructField`.\n",
    "* `StructField` is built using column name and data type. All the data types are available under `pyspark.sql.types`.\n",
    "* We need to pass table name and schema for `spark.catalog.createTable`.\n",
    "* We have to pass path along with name and schema for `spark.catalog.createExternalTable`.\n",
    "* We can use source to define file format along with applicable options. For example, if we want to create a table for CSV, then source will be csv and we can pass applicable options for CSV such as sep, header etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start spark context for this Notebook so that we can execute the code provided. You can sign up for our [10 node state of the art cluster/labs](https://labs.itversity.com/plans) to learn Spark SQL using our unique integrated LMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f'{username} | Python - Spark Metastore'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use CLIs, you can use Spark SQL using one of the 3 approaches.\n",
    "\n",
    "**Using Spark SQL**\n",
    "\n",
    "```\n",
    "spark2-sql \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Scala**\n",
    "\n",
    "```\n",
    "spark2-shell \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Pyspark**\n",
    "\n",
    "```\n",
    "pyspark2 \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.shuffle.partitions', '2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "* Create database by name **{username}_airtraffic** and create external table for **airport-codes.txt**.\n",
    "  * Data have header\n",
    "  * Fields in each record are delimited by a tab character.\n",
    "  * We can pass options such as sep, header, inferSchema etc to define the schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateExternalTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtableName\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Creates a table based on the dataset in a data source.\n",
       "\n",
       "It returns the DataFrame associated with the external table.\n",
       "\n",
       "The data source is specified by the ``source`` and a set of ``options``.\n",
       "If ``source`` is not specified, the default data source configured by\n",
       "``spark.sql.sources.default`` will be used.\n",
       "\n",
       "Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and\n",
       "created external table.\n",
       "\n",
       ":return: :class:`DataFrame`\n",
       "\n",
       ".. versionadded:: 2.0\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark-2.4.7-bin-hadoop2.7/python/pyspark/sql/catalog.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.catalog.createExternalTable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "username = getpass.getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {username}_airtraffic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase(f\"{username}_airtraffic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'itv002461_airtraffic'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To create external table, we need to have write permissions over the path which we want to use.\n",
    "* As we have only read permissions on **/public/airtraffic_all/airport-codes** we cannot use that path while creating external table.\n",
    "* Let us copy the data to **/user/`whoami`/airtraffic_all/airport-codes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   3 itv002461 supergroup      11411 2022-06-17 02:07 /user/itv002461/airtraffic_all/airport-codes/airport-codes-na.txt\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -mkdir -p /user/`whoami`/airtraffic_all\n",
    "hdfs dfs -cp -f /public/airtraffic_all/airport-codes /user/`whoami`/airtraffic_all\n",
    "hdfs dfs -ls /user/`whoami`/airtraffic_all/airport-codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yuma\tAZ\tUSA\tYUM\tCanada\tYZFLa\tYWK"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -tail /user/`whoami`/airtraffic_all/airport-codes/airport-codes-na.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "airport_codes_path = f'/user/{username}/airtraffic_all/airport-codes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('DROP TABLE IF EXISTS airport_codes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>City</th><th>State</th><th>Country</th><th>IATA</th></tr>\n",
       "<tr><td>Abbotsford</td><td>BC</td><td>Canada</td><td>YXX</td></tr>\n",
       "<tr><td>Aberdeen</td><td>SD</td><td>USA</td><td>ABR</td></tr>\n",
       "<tr><td>Abilene</td><td>TX</td><td>USA</td><td>ABI</td></tr>\n",
       "<tr><td>Akron</td><td>OH</td><td>USA</td><td>CAK</td></tr>\n",
       "<tr><td>Alamosa</td><td>CO</td><td>USA</td><td>ALS</td></tr>\n",
       "<tr><td>Albany</td><td>GA</td><td>USA</td><td>ABY</td></tr>\n",
       "<tr><td>Albany</td><td>NY</td><td>USA</td><td>ALB</td></tr>\n",
       "<tr><td>Albuquerque</td><td>NM</td><td>USA</td><td>ABQ</td></tr>\n",
       "<tr><td>Alexandria</td><td>LA</td><td>USA</td><td>AEX</td></tr>\n",
       "<tr><td>Allentown</td><td>PA</td><td>USA</td><td>ABE</td></tr>\n",
       "<tr><td>Alliance</td><td>NE</td><td>USA</td><td>AIA</td></tr>\n",
       "<tr><td>Alpena</td><td>MI</td><td>USA</td><td>APN</td></tr>\n",
       "<tr><td>Altoona</td><td>PA</td><td>USA</td><td>AOO</td></tr>\n",
       "<tr><td>Amarillo</td><td>TX</td><td>USA</td><td>AMA</td></tr>\n",
       "<tr><td>Anahim Lake</td><td>BC</td><td>Canada</td><td>YAA</td></tr>\n",
       "<tr><td>Anchorage</td><td>AK</td><td>USA</td><td>ANC</td></tr>\n",
       "<tr><td>Appleton</td><td>WI</td><td>USA</td><td>ATW</td></tr>\n",
       "<tr><td>Arviat</td><td>NWT</td><td>Canada</td><td>YEK</td></tr>\n",
       "<tr><td>Asheville</td><td>NC</td><td>USA</td><td>AVL</td></tr>\n",
       "<tr><td>Aspen</td><td>CO</td><td>USA</td><td>ASE</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+-----------+-----+-------+----+\n",
       "|       City|State|Country|IATA|\n",
       "+-----------+-----+-------+----+\n",
       "| Abbotsford|   BC| Canada| YXX|\n",
       "|   Aberdeen|   SD|    USA| ABR|\n",
       "|    Abilene|   TX|    USA| ABI|\n",
       "|      Akron|   OH|    USA| CAK|\n",
       "|    Alamosa|   CO|    USA| ALS|\n",
       "|     Albany|   GA|    USA| ABY|\n",
       "|     Albany|   NY|    USA| ALB|\n",
       "|Albuquerque|   NM|    USA| ABQ|\n",
       "| Alexandria|   LA|    USA| AEX|\n",
       "|  Allentown|   PA|    USA| ABE|\n",
       "|   Alliance|   NE|    USA| AIA|\n",
       "|     Alpena|   MI|    USA| APN|\n",
       "|    Altoona|   PA|    USA| AOO|\n",
       "|   Amarillo|   TX|    USA| AMA|\n",
       "|Anahim Lake|   BC| Canada| YAA|\n",
       "|  Anchorage|   AK|    USA| ANC|\n",
       "|   Appleton|   WI|    USA| ATW|\n",
       "|     Arviat|  NWT| Canada| YEK|\n",
       "|  Asheville|   NC|    USA| AVL|\n",
       "|      Aspen|   CO|    USA| ASE|\n",
       "+-----------+-----+-------+----+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog. \\\n",
    "    createExternalTable(\"airport_codes\",\n",
    "                        path=airport_codes_path,\n",
    "                        source=\"csv\",\n",
    "                        sep=\"\\t\",\n",
    "                        header=\"true\",\n",
    "                        inferSchema=\"true\"\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='airport_codes', database='itv002461_airtraffic', description=None, tableType='EXTERNAL', isTemporary=False)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------+----+\n",
      "|       City|State|Country|IATA|\n",
      "+-----------+-----+-------+----+\n",
      "| Abbotsford|   BC| Canada| YXX|\n",
      "|   Aberdeen|   SD|    USA| ABR|\n",
      "|    Abilene|   TX|    USA| ABI|\n",
      "|      Akron|   OH|    USA| CAK|\n",
      "|    Alamosa|   CO|    USA| ALS|\n",
      "|     Albany|   GA|    USA| ABY|\n",
      "|     Albany|   NY|    USA| ALB|\n",
      "|Albuquerque|   NM|    USA| ABQ|\n",
      "| Alexandria|   LA|    USA| AEX|\n",
      "|  Allentown|   PA|    USA| ABE|\n",
      "|   Alliance|   NE|    USA| AIA|\n",
      "|     Alpena|   MI|    USA| APN|\n",
      "|    Altoona|   PA|    USA| AOO|\n",
      "|   Amarillo|   TX|    USA| AMA|\n",
      "|Anahim Lake|   BC| Canada| YAA|\n",
      "|  Anchorage|   AK|    USA| ANC|\n",
      "|   Appleton|   WI|    USA| ATW|\n",
      "|     Arviat|  NWT| Canada| YEK|\n",
      "|  Asheville|   NC|    USA| AVL|\n",
      "|      Aspen|   CO|    USA| ASE|\n",
      "+-----------+-----+-------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"airport_codes\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                |comment|\n",
      "+----------------------------+-------------------------------------------------------------------------+-------+\n",
      "|City                        |string                                                                   |null   |\n",
      "|State                       |string                                                                   |null   |\n",
      "|Country                     |string                                                                   |null   |\n",
      "|IATA                        |string                                                                   |null   |\n",
      "|                            |                                                                         |       |\n",
      "|# Detailed Table Information|                                                                         |       |\n",
      "|Database                    |itv002461_airtraffic                                                     |       |\n",
      "|Table                       |airport_codes                                                            |       |\n",
      "|Owner                       |itv002461                                                                |       |\n",
      "|Created Time                |Fri Jun 17 02:07:27 EDT 2022                                             |       |\n",
      "|Last Access                 |Wed Dec 31 19:00:00 EST 1969                                             |       |\n",
      "|Created By                  |Spark 2.4.7                                                              |       |\n",
      "|Type                        |EXTERNAL                                                                 |       |\n",
      "|Provider                    |csv                                                                      |       |\n",
      "|Table Properties            |[transient_lastDdlTime=1655446047]                                       |       |\n",
      "|Location                    |hdfs://m01.itversity.com:9000/user/itv002461/airtraffic_all/airport-codes|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe                       |       |\n",
      "|InputFormat                 |org.apache.hadoop.mapred.SequenceFileInputFormat                         |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat                |       |\n",
      "|Storage Properties          |[serialization.format=1, inferSchema=true, sep=\t, header=true]           |       |\n",
      "+----------------------------+-------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('DESCRIBE FORMATTED airport_codes').show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column(name='City', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='State', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='Country', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='IATA', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listColumns('airport_codes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprimitivesAsString\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprefersDecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mallowComments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mallowUnquotedFieldNames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mallowSingleQuotes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mallowNumericLeadingZero\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mallowBackslashEscapingAnyCharacter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcolumnNameOfCorruptRecord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdateFormat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtimestampFormat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmultiLine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mallowUnquotedControlChars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlineSep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msamplingRatio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdropFieldIfAllNull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Loads JSON files and returns the results as a :class:`DataFrame`.\n",
       "\n",
       "`JSON Lines <http://jsonlines.org/>`_ (newline-delimited JSON) is supported by default.\n",
       "For JSON (one record per file), set the ``multiLine`` parameter to ``true``.\n",
       "\n",
       "If the ``schema`` parameter is not specified, this function goes\n",
       "through the input once to determine the input schema.\n",
       "\n",
       ":param path: string represents path to the JSON dataset, or a list of paths,\n",
       "             or RDD of Strings storing JSON objects.\n",
       ":param schema: an optional :class:`pyspark.sql.types.StructType` for the input schema or\n",
       "               a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
       ":param primitivesAsString: infers all primitive values as a string type. If None is set,\n",
       "                           it uses the default value, ``false``.\n",
       ":param prefersDecimal: infers all floating-point values as a decimal type. If the values\n",
       "                       do not fit in decimal, then it infers them as doubles. If None is\n",
       "                       set, it uses the default value, ``false``.\n",
       ":param allowComments: ignores Java/C++ style comment in JSON records. If None is set,\n",
       "                      it uses the default value, ``false``.\n",
       ":param allowUnquotedFieldNames: allows unquoted JSON field names. If None is set,\n",
       "                                it uses the default value, ``false``.\n",
       ":param allowSingleQuotes: allows single quotes in addition to double quotes. If None is\n",
       "                                set, it uses the default value, ``true``.\n",
       ":param allowNumericLeadingZero: allows leading zeros in numbers (e.g. 00012). If None is\n",
       "                                set, it uses the default value, ``false``.\n",
       ":param allowBackslashEscapingAnyCharacter: allows accepting quoting of all character\n",
       "                                           using backslash quoting mechanism. If None is\n",
       "                                           set, it uses the default value, ``false``.\n",
       ":param mode: allows a mode for dealing with corrupt records during parsing. If None is\n",
       "             set, it uses the default value, ``PERMISSIVE``.\n",
       "\n",
       "        * ``PERMISSIVE`` : when it meets a corrupted record, puts the malformed string                   into a field configured by ``columnNameOfCorruptRecord``, and sets other                   fields to ``null``. To keep corrupt records, an user can set a string type                   field named ``columnNameOfCorruptRecord`` in an user-defined schema. If a                   schema does not have the field, it drops corrupt records during parsing.                   When inferring a schema, it implicitly adds a ``columnNameOfCorruptRecord``                   field in an output schema.\n",
       "        *  ``DROPMALFORMED`` : ignores the whole corrupted records.\n",
       "        *  ``FAILFAST`` : throws an exception when it meets corrupted records.\n",
       "\n",
       ":param columnNameOfCorruptRecord: allows renaming the new field having malformed string\n",
       "                                  created by ``PERMISSIVE`` mode. This overrides\n",
       "                                  ``spark.sql.columnNameOfCorruptRecord``. If None is set,\n",
       "                                  it uses the value specified in\n",
       "                                  ``spark.sql.columnNameOfCorruptRecord``.\n",
       ":param dateFormat: sets the string that indicates a date format. Custom date formats\n",
       "                   follow the formats at ``java.text.SimpleDateFormat``. This\n",
       "                   applies to date type. If None is set, it uses the\n",
       "                   default value, ``yyyy-MM-dd``.\n",
       ":param timestampFormat: sets the string that indicates a timestamp format. Custom date\n",
       "                        formats follow the formats at ``java.text.SimpleDateFormat``.\n",
       "                        This applies to timestamp type. If None is set, it uses the\n",
       "                        default value, ``yyyy-MM-dd'T'HH:mm:ss.SSSXXX``.\n",
       ":param multiLine: parse one record, which may span multiple lines, per file. If None is\n",
       "                  set, it uses the default value, ``false``.\n",
       ":param allowUnquotedControlChars: allows JSON Strings to contain unquoted control\n",
       "                                  characters (ASCII characters with value less than 32,\n",
       "                                  including tab and line feed characters) or not.\n",
       ":param encoding: allows to forcibly set one of standard basic or extended encoding for\n",
       "                 the JSON files. For example UTF-16BE, UTF-32LE. If None is set,\n",
       "                 the encoding of input JSON will be detected automatically\n",
       "                 when the multiLine option is set to ``true``.\n",
       ":param lineSep: defines the line separator that should be used for parsing. If None is\n",
       "                set, it covers all ``\\r``, ``\\r\\n`` and ``\\n``.\n",
       ":param samplingRatio: defines fraction of input JSON objects used for schema inferring.\n",
       "                      If None is set, it uses the default value, ``1.0``.\n",
       ":param dropFieldIfAllNull: whether to ignore column of all null values or empty\n",
       "                           array/struct during schema inference. If None is set, it\n",
       "                           uses the default value, ``false``.\n",
       "\n",
       ">>> df1 = spark.read.json('python/test_support/sql/people.json')\n",
       ">>> df1.dtypes\n",
       "[('age', 'bigint'), ('name', 'string')]\n",
       ">>> rdd = sc.textFile('python/test_support/sql/people.json')\n",
       ">>> df2 = spark.read.json(rdd)\n",
       ">>> df2.dtypes\n",
       "[('age', 'bigint'), ('name', 'string')]\n",
       "\n",
       ".. versionadded:: 1.4\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark-2.4.7-bin-hadoop2.7/python/pyspark/sql/readwriter.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.read.json?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
